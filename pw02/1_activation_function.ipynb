{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "Activation functions are essential to artificial neural networks. They are used to compute the output of artificial neurons and therefore, the output of the network.\n",
    "Activation functions must be differentiable if network parameters are found using a learning algorithm like backpropagation.\n",
    "\n",
    "This notebook shows some examples of activation functions, and how their shape change with respect to the weight of the connections between neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of some activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear\n",
    "$$output = neta$$\n",
    "\n",
    "Sigmoid\n",
    "$$output = \\frac {1}{1 + e^{-neta}}$$\n",
    "\n",
    "Hyperbolic tangent\n",
    "$$output = \\frac {e^{neta} - e^{-neta}}{e^{neta} + e^{-neta}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(neta):\n",
    "    '''Linear activation function'''\n",
    "    output = neta\n",
    "    d_output = np.ones(len(neta))\n",
    "    return (output, d_output)\n",
    "\n",
    "def sigmoid(neta):\n",
    "    '''Sigmoidal activation function'''\n",
    "    output = 1 / (1 + np.exp(-neta))\n",
    "    d_output = output * (1 - output)\n",
    "    return (output, d_output)\n",
    "\n",
    "def htan(neta):\n",
    "    '''Hyperbolic tangent activation function'''\n",
    "    exp = np.exp(neta)\n",
    "    m_exp = np.exp(-neta)\n",
    "    output = (exp - m_exp ) / (exp + m_exp)\n",
    "    d_output = 1 - (output * output)\n",
    "    return (output, d_output)\n",
    "\n",
    "activation_functions_dict = {'Linear': linear, 'Sigmoid': sigmoid, 'Hyperbolic tangent': htan}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to plot the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = np.arange(-10, 10, 0.01)\n",
    "\n",
    "def plot_activation_function(activation_function_index, weight):\n",
    "    neta = weight * input_values\n",
    "    \n",
    "    activation_function = activation_functions_dict.get(list(activation_functions_dict.keys())[activation_function_index])\n",
    "    output_value, d_output_value = activation_function(neta)\n",
    "    \n",
    "    pl.figure(figsize=(8,6))\n",
    "    pl.plot(input_values, output_value, label='output')\n",
    "    pl.plot(input_values, weight * d_output_value, c='r', label='first derivative')\n",
    "    pl.xlabel('Input value')\n",
    "    pl.ylabel('Output value')\n",
    "    pl.ylim(-1.1, 1.1)\n",
    "    pl.legend(loc=4)\n",
    "    pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_slider = widgets.FloatSlider(\n",
    "    value=1.0,\n",
    "    min=-2.0,\n",
    "    max=2.0,\n",
    "    step=0.01,\n",
    "    description='Weight',\n",
    ")\n",
    "activation_function_list = widgets.Dropdown(\n",
    "    options={k:i for i,k in enumerate(activation_functions_dict.keys())},\n",
    "    value=1,\n",
    "    description='Activation function',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873168dd45bc48529801734f685fe34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Activation function', index=1, options={'Linear': 0, 'Sigmoid': 1,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(plot_activation_function, activation_function_index=activation_function_list, weight=weight_slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observe the shape of the different activation functions proposed.\n",
    "\n",
    "- Observe the effects of modifying the weight. How the shape of the function changes? How the first derivative changes?\n",
    "\n",
    "- Implement the activation function of a rectified Linear Unit (ReLU)\n",
    "\n",
    "$$ f(x) = \\left \\{\n",
    "\\begin{array}{rcl}\n",
    "\t0 & \\mbox{for} & x < 0\\\\\n",
    "\tx & \\mbox{for} & x \\ge 0\n",
    "\\end{array} \\right.\n",
    "\\hspace{1cm}\n",
    "f'(x) = \\left \\{\n",
    "\\begin{array}{rcl}\n",
    "\t0 & \\mbox{for} & x < 0\\\\\n",
    "\t1 & \\mbox{for} & x \\ge 0\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "- Visualize the ReLu activation function using the tools given in this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
