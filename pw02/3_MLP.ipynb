{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron\n",
    "A Multi-Layer Perceptron (MLP) is a collection of perceptrons connected and organized in layers. The output of one layer of perceptrons is the input of the next layer. Information travels through the network from input to output nodes. There are also special configurations in which the information goes back to previous layers thanks to recurrent connections. Recurrent connections are out of the scope of this laboratory. In this notebook you are going to test how the output of a MLP changes with respect to the changes in its weight connections for different activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, interactive, widgets\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of some activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear\n",
    "$$output = neta$$\n",
    "\n",
    "Sigmoid\n",
    "$$output = \\frac {1}{1 + e^{-neta}}$$\n",
    "\n",
    "Hyperbolic tangent\n",
    "$$output = \\frac {e^{neta} - e^{-neta}}{e^{neta} + e^{-neta}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(neta):\n",
    "    '''Linear activation function'''\n",
    "    output = neta\n",
    "    return output\n",
    "\n",
    "def sigmoid(neta):\n",
    "    ''' Sigmoidal activation function'''\n",
    "    output = 1 / (1 + np.exp(-neta))\n",
    "    return output\n",
    "\n",
    "def htan(neta):\n",
    "    '''Hyperbolic tangent activation function'''\n",
    "    exp = np.exp(neta)\n",
    "    m_exp = np.exp(-neta)\n",
    "    output = (exp - m_exp ) / (exp + m_exp)\n",
    "    return output\n",
    "\n",
    "activation_functions_dict = {'Linear': linear, 'Sigmoid': sigmoid, 'Hyperbolic tangent': htan}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "            ______________\n",
    "           /              \\\n",
    "x __w_x__ j                l\n",
    "  _______ | f_act(I.W + b) |----- output\n",
    "y   w_y   l                j\n",
    "           \\______________/\n",
    "Where:\n",
    "x = input x\n",
    "y = input y\n",
    "b = bias\n",
    "f_act = activation function\n",
    "I = vector of inputs [x, y]\n",
    "W = vector of weights [w_x, w_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$output = f\\_act(\\sum_{i=0}^{1}{(I_{i} * W_{i}) + b})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(input_values, weights, bias, activation_function):\n",
    "    '''Computes the output of a perceptron\n",
    "    :param input_values: inputs to the perceptron\n",
    "    :param weights: perceptron parameters (multiply inputs)\n",
    "    :param bias: perceptron parameter (adds to inputs)\n",
    "    :param activation_function: activation function to apply to the weighted sum of inputs\n",
    "    :return: perceptron output'''\n",
    "    neta = np.dot(input_values, weights) + bias\n",
    "    output = activation_function(neta)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                 _________________\n",
    "                /                 \\\n",
    "x _____w_x_0___j                   l   w_h_0\n",
    "     \\        _| f_act(I.W0 + b_0) |-----.\n",
    "      w_x_1  / l                   j     |      _________________\n",
    "        \\   /   \\_________________/      |     /                 \\\n",
    "         \\ /                           h0|____j                   l\n",
    "          \\                               ____| f_act(H.Wh + b_h) |------ output\n",
    "         / \\     _________________     h1|    l                   j\n",
    "    w_y_0   \\   /                 \\      |     \\_________________/\n",
    "      /      \\_j                   l     |\n",
    " ____/__w_y_1__| f_act(I.W1 + b_1) |-----'\n",
    "y              l                   j   w_h_1\n",
    "                \\_________________/\n",
    "           \n",
    "Where:\n",
    "x = input x\n",
    "y = input y\n",
    "b_0 = bias neuron 0\n",
    "b_1 = bias neuron 1\n",
    "b_h = bias output neuron\n",
    "f_act = activation function\n",
    "I = vector of inputs [x, y]\n",
    "H = vector of hidden activations [h0, h1]]\n",
    "W0 = vector of weights [w_x_0, w_y_0]\n",
    "W1 = vector of weights [w_x_1, w_y_1]\n",
    "Wh = vector of weights [w_h_0, w_h_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h0 = f\\_act(\\sum_{i=0}^{1}{(I_{i} * W0_{i}) + b\\_0})$$\n",
    "$$h1 = f\\_act(\\sum_{i=0}^{1}{(I_{i} * W1_{i}) + b\\_1})$$\n",
    "$$output = f\\_act(\\sum_{i=0}^{1}{(H_{i} * Wh_{i}) + b\\_h})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to plot the MLP output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = np.arange(-1.2, 1.2, 0.1)\n",
    "input_y = np.arange(-1.2, 1.2, 0.1)\n",
    "\n",
    "input_x_matrix, input_y_matrix = np.meshgrid(input_x, input_y)\n",
    "inputs_xy = np.concatenate((input_x_matrix.flatten()[:,np.newaxis], input_y_matrix.flatten()[:,np.newaxis]), axis=1)\n",
    "\n",
    "def plot_MLP(w_x_0, w_x_1, w_y_0, w_y_1, w_h_0, w_h_1, b_0, b_1, b_h, activation_function_index):\n",
    "    w_0 = np.array([w_x_0, w_y_0])\n",
    "    w_1 = np.array([w_x_1, w_y_1])\n",
    "    w_h = np.array([w_h_0, w_h_1])\n",
    "    \n",
    "    activation_function = activation_functions_dict.get(list(activation_functions_dict.keys())[activation_function_index])\n",
    "    \n",
    "    h_0 = perceptron(inputs_xy, w_0, b_0, activation_function)\n",
    "    h_1 = perceptron(inputs_xy, w_1, b_1, activation_function)\n",
    "    h = np.array([h_0, h_1]).T\n",
    "    \n",
    "    output_values = perceptron(h, w_h, b_h, activation_function)\n",
    "    output_matrix = np.reshape(output_values, input_x_matrix.shape)\n",
    "    \n",
    "    pl.figure(figsize=(8,6))\n",
    "    pl.imshow(np.flipud(output_matrix), interpolation='None', extent=(-1.2,1.2,-1.2,1.2), vmin=-1.0, vmax=1.0)\n",
    "    pl.xlabel('x')\n",
    "    pl.ylabel('y')\n",
    "    pl.colorbar()\n",
    "    pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_slider(name):\n",
    "    return widgets.FloatSlider(\n",
    "        value=0.5,\n",
    "        min=-2.0,\n",
    "        max=2.0,\n",
    "        step=0.01,\n",
    "        description=name,\n",
    "    )\n",
    "\n",
    "def create_controls():\n",
    "    controls = {name:create_slider(name) for name in ['w_x_0', 'w_x_1', 'w_y_0', 'w_y_1', 'w_h_0', 'w_h_1', 'b_0', 'b_1', 'b_h']}\n",
    "\n",
    "    controls['activation_function_index'] = widgets.Dropdown(\n",
    "        options={k:i for i,k in enumerate(activation_functions_dict.keys())},\n",
    "        value=1,\n",
    "        description='Activation function:',\n",
    "    )\n",
    "    return controls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the MLP output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72da02d3705348e599df3547b25bbd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='w_x_0', max=2.0, min=-2.0, step=0.01), FloatSlider(vâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "controls = create_controls()\n",
    "_= interact(plot_MLP, **controls);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the sliders to change the values of the connection weights and biases, and observe the resulting changes in the MLP output\n",
    "- Change the activation function and observe the changes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "widgets": {
   "state": {
    "639706f4b1114f73a07782e82a2f0f26": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "7c19e57a938644bca4e7ef98824106b5": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "84b7cb6dc4a04dc1b8c7bcc1c4362ea0": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "bc5bbb42d5144a80ba21bd5fc99342d5": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
