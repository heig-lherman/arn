{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "The topology of an artificial neural network has to be optimized to obtain the model wich generalizes the best. This notebook is an introduction to the process of model selection. During this laboratory, you will be able to learn how to use the tools for empirically finding the best topology of an artificial neural network and the parameters of the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "This function creates a dataset with two classes in two dimensions. It has two parameters: the size of the dataset and the spread of each one of the classes. A high spread value makes both classes to superpose, making the classification more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(n, s):\n",
    "    n1 = int(np.ceil(n / 2.0))\n",
    "    n2 = int(np.floor(n / 2.0))\n",
    "    x1 = np.random.normal(-1, s, n1)\n",
    "    y1 = np.random.uniform(-1, 1,  n1)\n",
    "    x2 = np.random.normal(1, s, n2)\n",
    "    y2 = np.random.uniform(-1, 1, n2)\n",
    "    return np.stack((np.concatenate((x1, x2)), np.concatenate((y1, y2)), np.concatenate((np.ones(n1), -1*np.ones(n2)))), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(s):\n",
    "    dataset = create_dataset(200, s)\n",
    "    pl.scatter(dataset[:,0], dataset[:,1], c=[(['b', 'r'])[int(cl > 0)] for cl in dataset[:,2]])\n",
    "    pl.xlim(-3,3)\n",
    "    pl.ylim(-1,1)\n",
    "    pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interact' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43minteract\u001B[49m(plot_dataset, s\u001B[38;5;241m=\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mFloatSlider(value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;28mmin\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;28mmax\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m, step\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m, description\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSpread:\u001B[39m\u001B[38;5;124m'\u001B[39m,));\n",
      "\u001B[0;31mNameError\u001B[0m: name 'interact' is not defined"
     ]
    }
   ],
   "source": [
    "interact(plot_dataset, s=widgets.FloatSlider(value=0.1, min=0.1, max=1.0, step=0.01, description='Spread:',));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MLP\n",
    "Import the code of the backpropagation with momentum algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlp_backprop_momentum as mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "Import the code of the cross-valdidation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import k_fold_cross_validation as cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Create a dataset to perform the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = 200\n",
    "SPREAD = 0.7\n",
    "dataset = create_dataset(DATASET_SIZE, SPREAD)\n",
    "\n",
    "pl.scatter(dataset[:,0], dataset[:,1], c=[(['b', 'r'])[int(cl > 0)] for cl in dataset[:,2]])\n",
    "pl.xlim(-3,3)\n",
    "pl.ylim(-1,1)\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the number of epochs\n",
    "Let's first try to estimate how many iterations of the backpropagation algorithm are enough to find a low training error. We fixed the learning rate to `0.001` and the momentum term to `0.5`. These values are just initial guesses which work well in most of the cases. Feel free to change these values if:\n",
    "\n",
    "+ the error curve oscillates -> reduce the learning rate\n",
    "+ the error curve is very smooth and does not change -> increase the learning rate\n",
    "+ the model does not converge -> try different values of momentum\n",
    "\n",
    "This initial test is performed by only tracking the training error. Hence, it is impossible to know if the model overfits or not. We will only know if the model's complexity is enough for learning the task.\n",
    "\n",
    "The following snippet also changes the number of hidden neurons from the simplest model possible (2 neurons) to a model having a complexity you think is excesive. Model selection in artificial neural networks is based on the experience. Moreover, notice that the MLPs are initialized several times to avoid a too bad result due to an unlucky initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INITS = 10\n",
    "EPOCHS = 200\n",
    "N_NEURONS = [2, 4, 8, 16, 32]\n",
    "LEARNING_RATE = 0.001\n",
    "MOMENTUM = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = np.zeros((len(N_NEURONS), N_INITS, EPOCHS))\n",
    "\n",
    "for i_h, h in enumerate(N_NEURONS):                                     # looping over the number of hidden neurons\n",
    "    print('Testing', h, 'neurons...')\n",
    "    nn = mlp.MLP([2,h,1], 'tanh')\n",
    "    for i in np.arange(N_INITS):                                        # looping over the initializations\n",
    "        nn.init_weights()\n",
    "        \n",
    "        MSE[i_h, i, :] = nn.fit((dataset[:,0:2], dataset[:,2:3]),\n",
    "                                learning_rate=LEARNING_RATE,\n",
    "                                momentum=MOMENTUM,\n",
    "                                epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(15,4))\n",
    "p_count = 0\n",
    "for n in np.arange(MSE.shape[0]):\n",
    "    pl.subplot(1, MSE.shape[0], n+1)\n",
    "    for i in np.arange(MSE.shape[1]):\n",
    "        pl.plot(MSE[n,i,:], c='b')\n",
    "    pl.ylim(0,1)\n",
    "    pl.xlabel('Epochs')\n",
    "    pl.ylabel('MSE')\n",
    "    pl.title(str(N_NEURONS[n]) + ' neurons')\n",
    "    pl.grid()\n",
    "pl.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In every case, from 2 to 32 neurons in the hidden layer, the training error does not improve anymore after 50 iterations. The result of this test indicates that the minimum number of iterations needed are around 20 iterations (Taking into account only the training error!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the number of hidden neurons\n",
    "Knowing that there are no significant improvements after 50 iterations, we can now further explore how the complexity of the model (number of hidden neurons) is linked to the generalization performance (test error). The following snippet allows you to explore both the number of epochs and the number of hidden neurons without restarting the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "K = 5\n",
    "N_TESTS = 10\n",
    "N_NEURONS = [2, 4, 6, 8, 10, 15, 20, 25, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_train = np.zeros((len(N_NEURONS), EPOCHS, N_TESTS))\n",
    "MSE_test = np.zeros((len(N_NEURONS), EPOCHS, N_TESTS))\n",
    "\n",
    "for i_h, h in enumerate(N_NEURONS):                                     # looping the number of hidden neurons\n",
    "    print('Testing', h, 'neurons...')\n",
    "    nn = mlp.MLP([2,h,1], 'tanh')\n",
    "    for i in np.arange(N_TESTS):                                        # looping the tests\n",
    "        nn.init_weights()                                               # the network has to be reinitialized before each test\n",
    "        temp1, temp2 = cv.k_fold_cross_validation_per_epoch(nn,         # notice that we do not use cv.k_fold_cross_validation\n",
    "                                                            dataset,    # but cv.k_fold_cross_validation_per_epoch which\n",
    "                                                            k=K,        # returns a value of error per each epoch\n",
    "                                                            learning_rate=LEARNING_RATE,\n",
    "                                                            momentum=MOMENTUM,\n",
    "                                                            epochs=EPOCHS)\n",
    "        # temp1 and temp2 are the training and test error. One value per epoch\n",
    "        MSE_train[i_h, :, i] = temp1\n",
    "        MSE_test[i_h, :, i] = temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_train_mean = np.mean(MSE_train, axis=2)\n",
    "MSE_test_mean = np.mean(MSE_test, axis=2)\n",
    "MSE_train_sd = np.std(MSE_train, axis=2)\n",
    "MSE_test_sd = np.std(MSE_test, axis=2)\n",
    "\n",
    "v_min = min(np.min(MSE_train_mean), np.min(MSE_test_mean))\n",
    "v_max = max(np.max(MSE_train_mean), np.max(MSE_test_mean))\n",
    "\n",
    "n_rows = int(np.ceil(len(N_NEURONS)/3.0))\n",
    "pl.figure(figsize=(12,3*n_rows))\n",
    "for i_n, n in enumerate(N_NEURONS):\n",
    "    pl.subplot(n_rows, min(3, len(N_NEURONS)), i_n+1)\n",
    "    pl.fill_between(np.arange(EPOCHS), MSE_train_mean[i_n,:], MSE_train_mean[i_n,:]+MSE_train_sd[i_n,:], facecolor='blue', alpha=0.5, label='Train')\n",
    "    pl.fill_between(np.arange(EPOCHS), MSE_train_mean[i_n,:], MSE_train_mean[i_n,:]-MSE_train_sd[i_n,:], facecolor='blue', alpha=0.5)\n",
    "    pl.fill_between(np.arange(EPOCHS), MSE_test_mean[i_n,:], MSE_test_mean[i_n,:]+MSE_test_sd[i_n,:], facecolor='red', alpha=0.5, label='Test')\n",
    "    pl.fill_between(np.arange(EPOCHS), MSE_test_mean[i_n,:], MSE_test_mean[i_n,:]-MSE_test_sd[i_n,:], facecolor='red', alpha=0.5)\n",
    "    pl.ylim(0.95*v_min,0.5*v_max)\n",
    "    pl.ylabel('MSE')\n",
    "    pl.xlabel('Number of epochs')\n",
    "    pl.title(str(K)+'-fold CV with '+str(n)+' hidden neurons')\n",
    "    pl.legend()\n",
    "    pl.grid()\n",
    "pl.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the test error does not improve much between 40 and 60 iterations, neither adding more hidden neurons after 4 units. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet shows a different visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(15,8))\n",
    "pl.subplot(2,1,1)\n",
    "pl.imshow(MSE_train_mean, vmin=np.min(MSE_train_mean), vmax=np.percentile(MSE_train_mean, 90), aspect=3, interpolation='nearest')\n",
    "pl.yticks(np.arange(len(N_NEURONS)), N_NEURONS)\n",
    "pl.xlabel('Epochs')\n",
    "pl.ylabel('Number of hidden Neurons')\n",
    "pl.title('Training')\n",
    "pl.colorbar()\n",
    "pl.subplot(2,1,2)\n",
    "pl.imshow(MSE_test_mean, vmin=np.min(MSE_test_mean), vmax=np.percentile(MSE_test_mean, 90), aspect=3, interpolation='nearest')\n",
    "pl.yticks(np.arange(len(N_NEURONS)), N_NEURONS)\n",
    "pl.xlabel('Epochs')\n",
    "pl.ylabel('Number of hidden Neurons')\n",
    "pl.title('Test')\n",
    "pl.colorbar()\n",
    "pl.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular problem, having more hidden neurons or running the backpropagation algorithm longer does not affect negatively the performance of the model. However, more iterations implies more processing time and more neurons implies more ressources and this should be avoided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The final model\n",
    "An artificial neural network with 2 neurons and 40 iterations of the backpropagation algorithm is enough to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = mlp.MLP([2,4,1], 'tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_train, MSE_test, conf_mat = cv.k_fold_cross_validation(nn,\n",
    "                                                          dataset,\n",
    "                                                          k=K,\n",
    "                                                          learning_rate=LEARNING_RATE,\n",
    "                                                          momentum=MOMENTUM,\n",
    "                                                          epochs=50,\n",
    "                                                          threshold=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE training: ', MSE_train)\n",
    "print('MSE test: ', MSE_test)\n",
    "print('Confusion matrix:')\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "state": {
    "f7df0ab69b604903958cf654849aa9ee": {
     "views": [
      {
       "cell_index": 6
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
